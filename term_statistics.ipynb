{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term-statistics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashhbeladiya/freegamescout/blob/main/term_statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yYCQrU0J5C5"
      },
      "source": [
        "# Working with Terms and Documents\n",
        "\n",
        "This exercise starts off with term statistics computations and graphing. In the final section (for CS6200 students), you collect new documents to experiment with.\n",
        "\n",
        "Read through this Jupyter notebook and fill in the parts marked with `TODO`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA4bCPS1MmAx"
      },
      "source": [
        "## Sample Data\n",
        "\n",
        "Start by looking at some sample data. We donwload the counts of terms in documents for the first one million tokens of a newswire collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-IMINS4IFUg",
        "outputId": "d47c95bc-3b6c-45f7-a10c-c9011adc2f56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget -O ap201001.json.gz https://github.com/dasmiq/cs6200-documents/blob/main/ap201001.json.gz?raw=true\n",
        "!gunzip ap201001.json.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-29 22:46:22--  https://github.com/dasmiq/cs6200-documents/blob/main/ap201001.json.gz?raw=true\n",
            "Resolving github.com (github.com)... 20.27.177.113\n",
            "Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/dasmiq/cs6200-documents/raw/refs/heads/main/ap201001.json.gz [following]\n",
            "--2025-01-29 22:46:22--  https://github.com/dasmiq/cs6200-documents/raw/refs/heads/main/ap201001.json.gz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dasmiq/cs6200-documents/refs/heads/main/ap201001.json.gz [following]\n",
            "--2025-01-29 22:46:23--  https://raw.githubusercontent.com/dasmiq/cs6200-documents/refs/heads/main/ap201001.json.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2854711 (2.7M) [application/octet-stream]\n",
            "Saving to: ‘ap201001.json.gz’\n",
            "\n",
            "ap201001.json.gz    100%[===================>]   2.72M  8.35MB/s    in 0.3s    \n",
            "\n",
            "2025-01-29 22:46:24 (8.35 MB/s) - ‘ap201001.json.gz’ saved [2854711/2854711]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SEFjGZvM4lY"
      },
      "source": [
        "We convert this file with one JSON record on each line to a list of dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CrLD5SOIMz1"
      },
      "source": [
        "import json\n",
        "rawfile = open('ap201001.json')\n",
        "terms = [json.loads(line) for line in rawfile]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg4P9XJJM_lZ"
      },
      "source": [
        "Here are the first ten records, showing the count of each term for each document and field. In this dataset, field only takes the values `body` or `title`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8GdOuyzI0wm",
        "outputId": "21700c53-ecdd-4077-e535-7e5e296206ac"
      },
      "source": [
        "terms[1:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'count': 1, 'field': 'body', 'id': 'APW_ENG_20100101.0001', 'term': 'about'},\n",
              " {'count': 1, 'field': 'body', 'id': 'APW_ENG_20100101.0001', 'term': 'abuse'},\n",
              " {'count': 1,\n",
              "  'field': 'body',\n",
              "  'id': 'APW_ENG_20100101.0001',\n",
              "  'term': 'academy'},\n",
              " {'count': 2,\n",
              "  'field': 'body',\n",
              "  'id': 'APW_ENG_20100101.0001',\n",
              "  'term': 'accused'},\n",
              " {'count': 1,\n",
              "  'field': 'body',\n",
              "  'id': 'APW_ENG_20100101.0001',\n",
              "  'term': 'actress'},\n",
              " {'count': 1, 'field': 'body', 'id': 'APW_ENG_20100101.0001', 'term': 'ad'},\n",
              " {'count': 1, 'field': 'body', 'id': 'APW_ENG_20100101.0001', 'term': 'after'},\n",
              " {'count': 1,\n",
              "  'field': 'body',\n",
              "  'id': 'APW_ENG_20100101.0001',\n",
              "  'term': 'agenda'},\n",
              " {'count': 1,\n",
              "  'field': 'body',\n",
              "  'id': 'APW_ENG_20100101.0001',\n",
              "  'term': 'agreed'}]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-CjFLXH3BJg"
      },
      "source": [
        "Each record has four fields:\n",
        "* `id`, with the identifier for the document;\n",
        "* `field`, with the region of the document containing a given term;\n",
        "* `term`, with the lower-cased term; and\n",
        "* `count`, with the number of times each term occurred in that field and document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H5yBvEVNUPr"
      },
      "source": [
        "## Computing Term Statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhDt23kKv0Uy"
      },
      "source": [
        "If we look at the most frequent terms for a given document, we mostly see common function words, such as `the`, `and`, and `of`. Start exploring the dataset by computing some of these basic term statistics. You can make your life easier using data frame libraries such as `pandas`, core python libraries such as `collections`, or just simple list comprehensions.\n",
        "\n",
        "Feel free to define helper functions in your code before computing the statistics we're looking for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zy5qR562nZ5"
      },
      "source": [
        "# TODO: Print the 10 terms from document APW_ENG_20100101.0001 with the highest count."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7OwCo0w5R1q"
      },
      "source": [
        "# TODO: Print the 10 terms with the highest total count in the corpus."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnNEUACW23Dd"
      },
      "source": [
        "Raw counts may not be the most informative statistic. One common improvement is to use *inverse document frequency*, the inverse of the proportion of documents that contain a given term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiUA502P2QkH"
      },
      "source": [
        "# TODO: Compute the number of distinct documents in the collection.\n",
        "N = 0\n",
        "\n",
        "# TODO: Compute the number of distinct documents each term appears in\n",
        "# and store in a dictionary.\n",
        "df = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XMPAKYNCq6Y"
      },
      "source": [
        "# TODO: Print the relative document frequency of 'the',\n",
        "# i.e., the number of documents that contain 'the' divided by N."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohFmwtc7Chy3"
      },
      "source": [
        "Empricially, we usually see better retrieval results if we rescale term frequency (within documents) and inverse document frequency (across documents) with the log function. Let the `tfidf` of term _t_ in document _d_ be:\n",
        "```\n",
        "tfidf(t, d) = log(count(t, d) + 1) * log(N / df(t))\n",
        "```\n",
        "\n",
        "Later in the course, we will show a probabilistic derivation of this quantity based on smoothing language models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmyj4v_uHdyo"
      },
      "source": [
        "# TODO: Compute the tf-idf value for each term in each document.\n",
        "# Take the raw term data and add a tfidf field to each record.\n",
        "tfidf_terms = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlXQmMO9HxH0"
      },
      "source": [
        "# TODO: Print the 20 term-document pairs with the highest tf-idf values."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f61xitl1IApl"
      },
      "source": [
        "## Plotting Term Distributions\n",
        "\n",
        "Besides frequencies and tf-idf values within documents, it is often helpful to look at the distrubitions of word frequencies in the whole collection. In class, we talk about the Zipf distribution of word rank versus frequency and Heaps' Law relating the number of distinct words to the number of tokens.\n",
        "\n",
        "We might examine these distributions to see, for instance, if an unexpectedly large number of very rare terms occurs, which might indicate noise added to our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsM5k1_5Jj7Y"
      },
      "source": [
        "# TODO: Compute a list of the distinct words in this collection and sort it in descending order of frequency.\n",
        "# Thus frequency[0] should contain the word \"the\" and the count 62216.\n",
        "frequency = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdtc14EULkxS"
      },
      "source": [
        "# TODO: Plot a graph of the log of the rank (starting at 1) on the x-axis,\n",
        "# against the log of the frequency on the y-axis. You may use the matplotlib\n",
        "# or other library."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute the number of tokens in the corpus.\n",
        "# Remember to count each occurrence of each word. For instance, the 62,216\n",
        "# instances of \"the\" will all count here.\n",
        "ntokens = 0"
      ],
      "metadata": {
        "id": "-WdHjFCSC7WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute the proportion of tokens made up by the top 10 most\n",
        "# frequent words."
      ],
      "metadata": {
        "id": "V_7wOcqKAz9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute the proportion of tokens made up by the words that occur\n",
        "# exactly once in this collection."
      ],
      "metadata": {
        "id": "uF-1VxcZBXMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdiUXaXZMFqT"
      },
      "source": [
        "## Acquiring New Documents (for CS6200)\n",
        "\n",
        "For this assignment so far, you've worked with data that's already been extracted, tokenized, and counted. In this final section, you'll explore acquiring new data.\n",
        "\n",
        "One common way of acquiring data is through application programming interfaces (APIs) to various databases. The Library of Congress's [_Chronicling America_](https://chroniclingamerica.loc.gov/) site aggregates digitized US newspapers from the past two hundred years, such as the [_Seattle Star_](https://chroniclingamerica.loc.gov/lccn/sn87093407/1925-01-17/ed-1/seq-1/) from 100 years ago.\n",
        "\n",
        "You can use [the API](https://chroniclingamerica.loc.gov/about/api/) to retrieve JSON data listing all issues of the _Seattle Star_: https://chroniclingamerica.loc.gov/lccn/sn87093407.json\n",
        "\n",
        "Note the list in the `issues` field. For example, here is the record for the January 17, 1925, issue: https://chroniclingamerica.loc.gov/lccn/sn87093407/1925-01-17/ed-1.json\n",
        "\n",
        "In that issue record, you'll see records for each page, e.g.: https://chroniclingamerica.loc.gov/lccn/sn87093407/1925-01-17/ed-1/seq-1.json\n",
        "\n",
        "And inside that page record, you'll see links to data about that page in various data formats, such as JPEG, PDF, and plain text, which is what we want here: https://chroniclingamerica.loc.gov/lccn/sn87093407/1925-01-17/ed-1/seq-1/ocr.txt\n",
        "\n",
        "This plain text was transcribed from the old page images using optical character recognition (OCR) models, and so contains errors.\n",
        "\n",
        "Your task is to acquire and analyze the issues of the _Seattle Star_ from the month of January, 1925, i.e., the issues with a date field that starts with `1925-01`. This should be about the same amount of data as the million words from the Associated Press you analyzed in the last section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGPVnXv2O6AN"
      },
      "source": [
        "**TODO**: Write code that calls the _Chronicling America_ API to download and extract the text from the _Seattle Star_ from January 1925. You can use the `json` library from above and any other libraries you wish to fetch data from URLs. As you would when working with any production API, you may need to limit your rate of requests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeDDz1SaPLik"
      },
      "source": [
        "# TODO: Data acquisition code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1-oRTixPKbw"
      },
      "source": [
        "**TODO**: Write code to tokenize the text and count the resulting terms in each document. Since this data comes from automatically transcribing printed pages, some words may be hyphenated across line breaks. There is more than one right way to tokenize this data, so add comments to your code documenting your choices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydsh0h74Pnlh"
      },
      "source": [
        "# TODO: Tokenization code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-boFCotQ4Ur"
      },
      "source": [
        "**TODO**: Plot a graph of the log rank against log frequency for your collection, as you did for the sample collection above."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Plotting code here."
      ],
      "metadata": {
        "id": "-Z35asgBjiqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: What do you observe about the differences between the distributions of the Associated Press and Seattle Star collections? In this text box, give some possible reasons for these differences."
      ],
      "metadata": {
        "id": "D5B-h__CjpvI"
      }
    }
  ]
}